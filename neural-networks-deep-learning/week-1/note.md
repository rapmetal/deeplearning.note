神经网络和深度学习(Neural Networks and Deep Learning)
==============================================================
### 1.1 欢迎(Welcome)

这一节课相对比较简单，这里讲了深度学习的价值以及能做什么事情。
我这边顺便给大家回顾一下“机器学习”的一些基本知识，可能对你理解和深度学习的异同有帮助。上来仍然抛出一个比较好的引用：

[简谈机器学习](https://www.cnblogs.com/subconscious/p/4107357.html) 

机器学习方法是计算机利用已有的数据(经验)，得出了某种模型(规律)，并利用此模型预测未来(结论概率)的一种方法。

#### 常见的机器学习方法：

1. 线性回归
        
    一般会用[最小二乘法]来解决。简单说就是为了达到以下的公式为目的，使得所有误差的平方和最小。
    ![](../../imgs/least_squares.svg)
    
    - 最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。最小二乘法还可用于曲线拟合。其他一些优化问题也可通过最小化能量或最大化熵用最小二乘法来表达。
    
        其实根本不用把最小二乘法想的多么高大上，不就是求极值嘛。学过大学高等数学的人应该都知道求极值的方法：就是求偏导，然后使偏导为0，这就是最小二乘法整个的方法了，so easy。
        ![](../../imgs/least_squares-2.svg)
        最后使所有的偏导等于0
        ![](../../imgs/least_squares-3.svg)
        然后解这个方程组就可以得到各个系数的值了。

    然而是否可以用最小平方来衡量loss函数的合理性，可以用最大似然来衡量。
    
    [极大似然估计详解](https://blog.csdn.net/zengxiantao1994/article/details/72787849)

    - 梯度下降法 


2. 逻辑回归
    
    逻辑回归是一种与线性回归非常类似的算法，但是，从本质上讲，线型回归处理的问题类型与逻辑回归不一致。线性回归处理的是数值问题，也就是最后预测出的结果是数字，例如房价。而逻辑回归属于分类算法，也就是说，逻辑回归预测结果是离散的分类，例如判断这封邮件是否是垃圾邮件，以及用户是否会点击此广告等等。

    实现方面的话，逻辑回归只是对对线性回归的计算结果加上了一个Sigmoid函数，将数值结果转化为了0到1之间的概率(Sigmoid函数的图像一般来说并不直观，你只需要理解对数值越大，函数越逼近1，数值越小，函数越逼近0)，接着我们根据这个概率可以做预测，例如概率大于0.5，则这封邮件就是垃圾邮件，或者肿瘤是否是恶性的等等。从直观上来说，逻辑回归是画出了一条分类线，见下图。



#### 关于深度学习

    感兴趣的可以自行搜索一些材料，这里给出我觉得比较好的一些引用。

    [维基百科](https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0)

    [5分钟搞懂什么是深度学习](https://www.jianshu.com/p/27e9f1451882)

### 1.2 什么是神经网络？(What is a Neural Network)

我们常常用深度学习这个术语来指训练神经网络的过程。有时它指的是特别大规模的神经网络训练。啥是神经网络，这里再给一个完整的引用可以讲清楚关于神经网络的前世今生。

神经网络浅讲：从神经元到深度学习：https://www.cnblogs.com/subconscious/p/5058741.html

